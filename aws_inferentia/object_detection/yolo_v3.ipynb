{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate YOLO v3 on Inferentia\n",
    "## Note: this tutorial runs on tensorflow-neuron 1.x only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial walks through compiling and evaluating YOLO v3 model on Inferentia using the AWS Neuron SDK.\n",
    "\n",
    "\n",
    "In this tutorial we provide two main sections:\n",
    "\n",
    "1. Download Dataset and Generate Pretrained SavedModel\n",
    "\n",
    "2. Compile the YOLO v3 model.\n",
    "\n",
    "3. Deploy the same compiled model.\n",
    "\n",
    "Before running the following verify this Jupyter notebook is running “conda_aws_neuron_tensorflow_p36” kernel. You can select the Kernel from the “Kernel -> Change Kernel” option on the top of this Jupyter notebook page.\n",
    "\n",
    "Instructions of how to setup Neuron Tensorflow environment and run the tutorial as a Jupyter notebook are available in the Tutorial main page [Tensorflow-YOLO_v3 Tutorial](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/tensorflow-neuron/tutorials/yolo_v3_demo/yolo_v3_demo.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo requires the following pip packages:\n",
    "\n",
    "`pillow matplotlib pycocotools`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/ubuntu/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting pillow\n",
      "  Downloading Pillow-8.3.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 144.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycocotools==2.0.2\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Collecting setuptools>=18.0\n",
      "  Downloading setuptools-58.1.0-py3-none-any.whl (816 kB)\n",
      "\u001b[K     |████████████████████████████████| 816 kB 113.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cython>=0.27.3\n",
      "  Downloading Cython-0.29.24-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 137.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[K     |████████████████████████████████| 247 kB 133.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 140.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 106.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting numpy>=1.15\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 132.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp36-cp36m-linux_x86_64.whl size=273279 sha256=d8c3fcfe0a662f3343253bfdbb963abd4911801fb8b00a531f161660eb69691f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gkluu9mo/wheels/d8/c2/ba/8f5306f921c2e79ad7b09effdfed6bd966cfcf8c6fe55422d6\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: six, python-dateutil, pyparsing, pillow, numpy, kiwisolver, cycler, setuptools, matplotlib, cython, pycocotools\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 2.4.7\n",
      "    Uninstalling pyparsing-2.4.7:\n",
      "      Successfully uninstalled pyparsing-2.4.7\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 8.3.2\n",
      "    Uninstalling Pillow-8.3.2:\n",
      "      Successfully uninstalled Pillow-8.3.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.3.1\n",
      "    Uninstalling kiwisolver-1.3.1:\n",
      "      Successfully uninstalled kiwisolver-1.3.1\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.10.0\n",
      "    Uninstalling cycler-0.10.0:\n",
      "      Successfully uninstalled cycler-0.10.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 58.0.4\n",
      "    Uninstalling setuptools-58.0.4:\n",
      "      Successfully uninstalled setuptools-58.0.4\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.4\n",
      "    Uninstalling matplotlib-3.3.4:\n",
      "      Successfully uninstalled matplotlib-3.3.4\n",
      "  Attempting uninstall: cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "  Attempting uninstall: pycocotools\n",
      "    Found existing installation: pycocotools 2.0.2\n",
      "    Uninstalling pycocotools-2.0.2:\n",
      "      Successfully uninstalled pycocotools-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 1.15.5 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "Successfully installed cycler-0.10.0 cython-0.29.24 kiwisolver-1.3.1 matplotlib-3.3.4 numpy-1.19.5 pillow-8.3.2 pycocotools-2.0.2 pyparsing-2.4.7 python-dateutil-2.8.2 setuptools-58.1.0 six-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pillow matplotlib pycocotools==2.0.2 --force --extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Download Dataset and Generate Pretrained SavedModel\n",
    "### Download COCO 2017 validation dataset\n",
    "\n",
    "We start by downloading the COCO validation dataset, which we will use to validate our model. The COCO 2017 dataset is widely used for object-detection, segmentation and image captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  777M  100  777M    0     0  22.1M      0  0:00:35  0:00:35 --:--:-- 22.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  241M  100  241M    0     0  21.7M      0  0:00:11  0:00:11 --:--:-- 22.8M\n",
      "replace val2017/000000212226.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!curl -LO http://images.cocodataset.org/zips/val2017.zip\n",
    "!curl -LO http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip -q val2017.zip\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Generate YOLO v3 tensorflow SavedModel (pretrained on COCO 2017 dataset)\n",
    "\n",
    "Script yolo_v3_coco_saved_model.py will generate a tensorflow SavedModel using pretrained weights from https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run yolo_v3_coco_saved_model.py ./yolo_v3_coco_saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensorflow SavedModel can be loaded as a tensorflow predictor. When a JPEG format image is provided as input, the output result of the tensorflow predictor contains information for drawing bounding boxes and classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# launch predictor and run inference on an arbitrary image in the validation dataset\n",
    "yolo_pred_cpu = tf.contrib.predictor.from_saved_model('./yolo_v3_coco_saved_model')\n",
    "image_path = './val2017/000000581781.jpg'\n",
    "with open(image_path, 'rb') as f:\n",
    "    feeds = {'image': [f.read()]}\n",
    "results = yolo_pred_cpu(feeds)\n",
    "\n",
    "# load annotations to decode classification result\n",
    "with open('./annotations/instances_val2017.json') as f:\n",
    "    annotate_json = json.load(f)\n",
    "label_info = {idx+1: cat['name'] for idx, cat in enumerate(annotate_json['categories'])}\n",
    "\n",
    "# draw picture and bounding boxes\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(Image.open(image_path).convert('RGB'))\n",
    "wanted = results['scores'][0] > 0.1\n",
    "for xyxy, label_no_bg in zip(results['boxes'][0][wanted], results['classes'][0][wanted]):\n",
    "    xywh = xyxy[0], xyxy[1], xyxy[2] - xyxy[0], xyxy[3] - xyxy[1]\n",
    "    rect = patches.Rectangle((xywh[0], xywh[1]), xywh[2], xywh[3], linewidth=1, edgecolor='g', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    rx, ry = rect.get_xy()\n",
    "    rx = rx + rect.get_width() / 2.0\n",
    "    ax.annotate(label_info[label_no_bg + 1], (rx, ry), color='w', backgroundcolor='g', fontsize=10,\n",
    "                ha='center', va='center', bbox=dict(boxstyle='square,pad=0.01', fc='g', ec='none', alpha=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Compile the Pretrained SavedModel for Neuron\n",
    "\n",
    "We make use of the Python compilation API `tfn.saved_model.compile` that is available in `tensorflow-neuron<2`. For the purpose of reducing Neuron runtime overhead, it is necessary to make use of arguments `no_fuse_ops` and `minimum_segment_size`.\n",
    "Compiled model is saved in ./yolo_v3_coco_saved_model_neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-097c7aa18dd8>:10: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from ./yolo_v3_coco_saved_model/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from ./yolo_v3_coco_saved_model/variables/variables\n",
      "INFO:tensorflow:Froze 366 variables.\n",
      "INFO:tensorflow:Converted 366 variables to const ops.\n",
      "INFO:tensorflow:fusing subgraph {subgraph neuron_op_40079fd99a167dfc with input tensors [\"<tf.Tensor 'Preprocessor/map/TensorArrayStack/TensorArrayGatherV30/_0:0' shape=(1, 416, 416, 3) dtype=float16>\"], output tensors [\"<tf.Tensor 'conv_lbbox/BiasAdd:0' shape=(1, 13, 13, 255) dtype=float16>\", \"<tf.Tensor 'conv_mbbox/BiasAdd:0' shape=(1, 26, 26, 255) dtype=float16>\", \"<tf.Tensor 'conv_sbbox/BiasAdd:0' shape=(1, 52, 52, 255) dtype=float16>\"]} with neuron-cc\n",
      "INFO:tensorflow:Number of operations in TensorFlow session: 4962\n",
      "INFO:tensorflow:Number of operations after tf.neuron optimizations: 3010\n",
      "INFO:tensorflow:Number of operations placed on Neuron runtime: 1891\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./yolo_v3_coco_inf1_saved_models/saved_model.pb\n",
      "INFO:tensorflow:Successfully converted ./yolo_v3_coco_saved_model to ./yolo_v3_coco_inf1_saved_models\n",
      "{'OnNeuronRatio': 0.6282392026578073}\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow.neuron as tfn\n",
    "import os\n",
    "\n",
    "model_type = 'yolo_v3_coco'\n",
    "\n",
    "def no_fuse_condition(op):\n",
    "    return op.name.startswith('Preprocessor') or op.name.startswith('Postprocessor')\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.saved_model.loader.load(sess, ['serve'], './yolo_v3_coco_saved_model')\n",
    "    no_fuse_ops = [op.name for op in sess.graph.get_operations() if no_fuse_condition(op)]\n",
    "def compile_inf1_model(saved_model_dir, inf1_model_dir, batch_size=1, num_cores=1, use_static_weights=False):\n",
    "    \n",
    "    compiled_model_dir = f'{model_type}_batch_{batch_size}_inf1_cores_{num_cores}'\n",
    "    inf1_compiled_model_dir = os.path.join(inf1_model_dir, compiled_model_dir)\n",
    "    shutil.rmtree(inf1_compiled_model_dir, ignore_errors=True)\n",
    "    \n",
    "    compiler_args = ['--verbose','1', '--neuroncore-pipeline-cores', str(num_cores)]\n",
    "    \n",
    "    result = tfn.saved_model.compile(\n",
    "        saved_model_dir, compiled_model_dir,\n",
    "        # to enforce trivial compilable subgraphs to run on CPU\n",
    "    #     no_fuse_ops=no_fuse_ops,\n",
    "        minimum_segment_size=100,\n",
    "        batch_size=batch_size,\n",
    "        dynamic_batch_size=True,\n",
    "        compiler_args = compiler_args\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inf1_model_dir = f'{model_type}_inf1_saved_models'\n",
    "saved_model_dir = f'{model_type}_saved_model'\n",
    "\n",
    "\n",
    "# testing batch size\n",
    "batch_list = [1,2,4,8,16,32,64]\n",
    "num_of_cores = [1,2,3,4]\n",
    "for batch in batch_list:\n",
    "    for core in num_of_cores:\n",
    "        print('batch size:', batch,'core nums', core,'compile start')\n",
    "        compile_inf1_model(saved_model_dir, inf1_model_dir, batch_size=batch, num_cores=core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model on Inferentia\n",
    "## Part 3:Evaluate Model Quality after Compilation\n",
    "\n",
    "### Define evaluation functions\n",
    "We first define some handy helper functions for running evaluation on the COCO 2017 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cocoapi_eval(jsonfile,\n",
    "                 style,\n",
    "                 coco_gt=None,\n",
    "                 anno_file=None,\n",
    "                 max_dets=(100, 300, 1000)):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        jsonfile: Evaluation json file, eg: bbox.json, mask.json.\n",
    "        style: COCOeval style, can be `bbox` , `segm` and `proposal`.\n",
    "        coco_gt: Whether to load COCOAPI through anno_file,\n",
    "                 eg: coco_gt = COCO(anno_file)\n",
    "        anno_file: COCO annotations file.\n",
    "        max_dets: COCO evaluation maxDets.\n",
    "    \"\"\"\n",
    "    assert coco_gt is not None or anno_file is not None\n",
    "\n",
    "    if coco_gt is None:\n",
    "        coco_gt = COCO(anno_file)\n",
    "    print(\"Start evaluate...\")\n",
    "    coco_dt = coco_gt.loadRes(jsonfile)\n",
    "    if style == 'proposal':\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "        coco_eval.params.useCats = 0\n",
    "        coco_eval.params.maxDets = list(max_dets)\n",
    "    else:\n",
    "        coco_eval = COCOeval(coco_gt, coco_dt, style)\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    return coco_eval.stats\n",
    "\n",
    "\n",
    "def bbox_eval(anno_file, bbox_list):\n",
    "    coco_gt = COCO(anno_file)\n",
    "\n",
    "    outfile = 'bbox_detections.json'\n",
    "    print('Generating json file...')\n",
    "    with open(outfile, 'w') as f:\n",
    "        json.dump(bbox_list, f)\n",
    "\n",
    "    map_stats = cocoapi_eval(outfile, 'bbox', coco_gt=coco_gt)\n",
    "    return map_stats\n",
    "\n",
    "\n",
    "def get_image_as_bytes(images, eval_pre_path):\n",
    "    batch_im_id_list = []\n",
    "    batch_im_name_list = []\n",
    "    batch_img_bytes_list = []\n",
    "    n = len(images)\n",
    "    batch_im_id = []\n",
    "    batch_im_name = []\n",
    "    batch_img_bytes = []\n",
    "    for i, im in enumerate(images):\n",
    "        im_id = im['id']\n",
    "        file_name = im['file_name']\n",
    "        if i % eval_batch_size == 0 and i != 0:\n",
    "            batch_im_id_list.append(batch_im_id)\n",
    "            batch_im_name_list.append(batch_im_name)\n",
    "            batch_img_bytes_list.append(batch_img_bytes)\n",
    "            batch_im_id = []\n",
    "            batch_im_name = []\n",
    "            batch_img_bytes = []\n",
    "        batch_im_id.append(im_id)\n",
    "        batch_im_name.append(file_name)\n",
    "\n",
    "        with open(os.path.join(eval_pre_path, file_name), 'rb') as f:\n",
    "            batch_img_bytes.append(f.read())\n",
    "    return batch_im_id_list, batch_im_name_list, batch_img_bytes_list\n",
    "\n",
    "\n",
    "def analyze_bbox(results, batch_im_id, _clsid2catid):\n",
    "    bbox_list = []\n",
    "    k = 0\n",
    "    for boxes, scores, classes in zip(results['boxes'], results['scores'], results['classes']):\n",
    "        if boxes is not None:\n",
    "            im_id = batch_im_id[k]\n",
    "            n = len(boxes)\n",
    "            for p in range(n):\n",
    "                clsid = classes[p]\n",
    "                score = scores[p]\n",
    "                xmin, ymin, xmax, ymax = boxes[p]\n",
    "                catid = (_clsid2catid[int(clsid)])\n",
    "                w = xmax - xmin + 1\n",
    "                h = ymax - ymin + 1\n",
    "\n",
    "                bbox = [xmin, ymin, w, h]\n",
    "                # Round to the nearest 10th to avoid huge file sizes, as COCO suggests\n",
    "                bbox = [round(float(x) * 10) / 10 for x in bbox]\n",
    "                bbox_res = {\n",
    "                    'image_id': im_id,\n",
    "                    'category_id': catid,\n",
    "                    'bbox': bbox,\n",
    "                    'score': float(score),\n",
    "                }\n",
    "                bbox_list.append(bbox_res)\n",
    "        k += 1\n",
    "    return bbox_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the actual evaluation loop. To fully utilize all four cores on one Inferentia, the optimal setup is to run multi-threaded inference using a `ThreadPoolExecutor`. The following cell is a multi-threaded adaptation of the evaluation routine at https://github.com/miemie2013/Keras-YOLOv4/blob/910c4c6f7265f5828fceed0f784496a0b46516bf/tools/cocotools.py#L97."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "def evaluate(yolo_predictor, images, eval_pre_path, anno_file, eval_batch_size, _clsid2catid, user_batch_size, num_cores):\n",
    "    batch_im_id_list, batch_im_name_list, batch_img_bytes_list = get_image_as_bytes(images, eval_pre_path)\n",
    "    walltime_start = time.time()\n",
    "    # warm up\n",
    "#     yolo_predictor({'image': np.array(batch_img_bytes_list[0], dtype=object)})\n",
    "\n",
    "#     with futures.ThreadPoolExecutor(4) as exe:\n",
    "#         fut_im_list = []\n",
    "#         fut_list = []\n",
    "#         start_time = time.time()\n",
    "#         for batch_im_id, batch_im_name, batch_img_bytes in zip(batch_im_id_list, batch_im_name_list, batch_img_bytes_list):\n",
    "#             if len(batch_img_bytes) != eval_batch_size:\n",
    "#                 continue\n",
    "#             fut = exe.submit(yolo_predictor, {'image': np.array(batch_img_bytes, dtype=object)})\n",
    "#             fut_im_list.append((batch_im_id, batch_im_name))\n",
    "#             fut_list.append(fut)\n",
    "#         bbox_list = []\n",
    "#         count = 0\n",
    "#         for (batch_im_id, batch_im_name), fut in zip(fut_im_list, fut_list):\n",
    "#             results = fut.result()\n",
    "#             bbox_list.extend(analyze_bbox(results, batch_im_id, _clsid2catid))\n",
    "#             for _ in batch_im_id:\n",
    "#                 count += 1\n",
    "#                 if count % 100 == 0:\n",
    "#                     print('Test iter {}'.format(count))\n",
    "#         print('==================== Performance Measurement ====================')\n",
    "#         print('Finished inference on {} images in {} seconds'.format(len(images), time.time() - start_time))\n",
    "#         print('=================================================================')\n",
    "#     # start evaluation\n",
    "#     box_ap_stats = bbox_eval(anno_file, bbox_list)\n",
    "    iter_times = []\n",
    "    counter = 0\n",
    "    first_iter_time = 0\n",
    "    fut_im_list = []\n",
    "    fut_list = []\n",
    "    for batch_im_id, batch_im_name, batch_img_bytes in zip(batch_im_id_list, batch_im_name_list, batch_img_bytes_list):\n",
    "        if len(batch_img_bytes) != eval_batch_size:\n",
    "            continue\n",
    "        iter_start = time.time()\n",
    "        fut = yolo_predictor({'image': np.array(batch_img_bytes, dtype=object)})\n",
    "        fut_im_list.append((batch_im_id, batch_im_name))\n",
    "        fut_list.append(fut)\n",
    "        if counter == 0:\n",
    "            first_iter_time = time.time() - iter_start\n",
    "        else:\n",
    "            iter_times.append(time.time() - iter_start)\n",
    "        counter +=1\n",
    "    bbox_list = []\n",
    "    counter = 0\n",
    "    for (batch_im_id, batch_im_name), fut in zip(fut_im_list, fut_list):\n",
    "        results = fut\n",
    "        bbox_list.extend(analyze_bbox(results, batch_im_id, _clsid2catid))\n",
    "        for _ in batch_im_id:\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print('Test iter {}'.format(counter))\n",
    "    \n",
    "    print('==================== Performance Measurement ====================')\n",
    "    print('Finished inference on {} images in {} seconds'.format(len(images), time.time() - walltime_start))\n",
    "    print('=================================================================')\n",
    "    \n",
    "    results = pd.DataFrame(columns = [f'inf1_compiled_batch_size_{eval_batch_size}_compiled_cores_{num_cores}'])\n",
    "    results.loc['compiled_batch_size'] = [eval_batch_size]\n",
    "    results.loc['user_batch_size'] = [user_batch_size]\n",
    "    results.loc['first_prediction_time'] = [first_iter_time]\n",
    "    results.loc['average_prediction_time'] = [np.mean(iter_times)]\n",
    "    results.loc['wall_time'] = [time.time() - walltime_start]\n",
    "    box_ap_stats = bbox_eval(anno_file, bbox_list)\n",
    "    return box_ap_stats, results, iter_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate mean average precision (mAP) score\n",
    "Here is the code to calculate mAP scores of the YOLO v3 model. The expected mAP score is around 0.328 if we use the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "Test iter 100\n",
      "Test iter 200\n",
      "Test iter 300\n",
      "Test iter 400\n",
      "Test iter 500\n",
      "Test iter 600\n",
      "Test iter 700\n",
      "Test iter 800\n",
      "Test iter 900\n",
      "Test iter 1000\n",
      "Test iter 1100\n",
      "Test iter 1200\n",
      "Test iter 1300\n",
      "Test iter 1400\n",
      "Test iter 1500\n",
      "Test iter 1600\n",
      "Test iter 1700\n",
      "Test iter 1800\n",
      "Test iter 1900\n",
      "Test iter 2000\n",
      "Test iter 2100\n",
      "Test iter 2200\n",
      "Test iter 2300\n",
      "Test iter 2400\n",
      "Test iter 2500\n",
      "Test iter 2600\n",
      "Test iter 2700\n",
      "Test iter 2800\n",
      "Test iter 2900\n",
      "Test iter 3000\n",
      "Test iter 3100\n",
      "Test iter 3200\n",
      "Test iter 3300\n",
      "Test iter 3400\n",
      "Test iter 3500\n",
      "Test iter 3600\n",
      "Test iter 3700\n",
      "Test iter 3800\n",
      "Test iter 3900\n",
      "Test iter 4000\n",
      "Test iter 4100\n",
      "Test iter 4200\n",
      "Test iter 4300\n",
      "Test iter 4400\n",
      "Test iter 4500\n",
      "Test iter 4600\n",
      "Test iter 4700\n",
      "Test iter 4800\n",
      "Test iter 4900\n",
      "==================== Performance Measurement ====================\n",
      "Finished inference on 5000 images in 49.936264514923096 seconds\n",
      "=================================================================\n",
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n",
      "Generating json file...\n",
      "Start evaluate...\n",
      "Loading and preparing results...\n",
      "DONE (t=2.28s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=36.37s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.94s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.558\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.349\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.139\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.364\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.266\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.371\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.375\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.160\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.571\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inf1_compiled_batch_size_8_compiled_cores_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>compiled_batch_size</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usr_batch_size</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_prediction_time</th>\n",
       "      <td>4.54269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_prediction_time</th>\n",
       "      <td>0.0659854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall_time</th>\n",
       "      <td>49.9435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        inf1_compiled_batch_size_8_compiled_cores_1\n",
       "compiled_batch_size                                               8\n",
       "usr_batch_size                                                    8\n",
       "first_prediction_time                                       4.54269\n",
       "average_prediction_time                                   0.0659854\n",
       "wall_time                                                   49.9435"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yolo_pred = tf.contrib.predictor.from_saved_model('./yolo_v3_coco_inf1_saved_models')\n",
    "\n",
    "val_coco_root = './val2017'\n",
    "val_annotate = './annotations/instances_val2017.json'\n",
    "clsid2catid = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 13, 12: 14, 13: 15, 14: 16,\n",
    "               15: 17, 16: 18, 17: 19, 18: 20, 19: 21, 20: 22, 21: 23, 22: 24, 23: 25, 24: 27, 25: 28, 26: 31,\n",
    "               27: 32, 28: 33, 29: 34, 30: 35, 31: 36, 32: 37, 33: 38, 34: 39, 35: 40, 36: 41, 37: 42, 38: 43,\n",
    "               39: 44, 40: 46, 41: 47, 42: 48, 43: 49, 44: 50, 45: 51, 46: 52, 47: 53, 48: 54, 49: 55, 50: 56,\n",
    "               51: 57, 52: 58, 53: 59, 54: 60, 55: 61, 56: 62, 57: 63, 58: 64, 59: 65, 60: 67, 61: 70, 62: 72,\n",
    "               63: 73, 64: 74, 65: 75, 66: 76, 67: 77, 68: 78, 69: 79, 70: 80, 71: 81, 72: 82, 73: 84, 74: 85,\n",
    "               75: 86, 76: 87, 77: 88, 78: 89, 79: 90}\n",
    "\n",
    "model_type = 'yolo_v3_coco'\n",
    "\n",
    "batch_list = [1]\n",
    "num_of_cores = [1]\n",
    "user_batchs = [64]\n",
    "inf1_model_dir = f'{model_type}_inf1_saved_models'\n",
    "for user_batch in user_batchs:\n",
    "    iter_ds = pd.DataFrame()\n",
    "    results = pd.DataFrame()\n",
    "    for batch_size in batch_list:\n",
    "        for num_cores in num_of_cores:\n",
    "            opt ={'batch_size': batch_size, 'num_cores': num_of_cores}\n",
    "            compiled_model_dir = f'{model_type}_batch_{batch_size}_inf1_cores_{num_cores}'\n",
    "            inf1_compiled_model_dir = os.path.join(inf1_model_dir, compiled_model_dir)\n",
    "\n",
    "            print(f'inf1_compiled_model_dir: {inf1_compiled_model_dir}')\n",
    "            col_name = lambda opt: f'inf1_{batch_size}_multicores_{num_cores}'\n",
    "\n",
    "            with open(val_annotate, 'r', encoding='utf-8') as f2:\n",
    "                for line in f2:\n",
    "                    line = line.strip()\n",
    "                    dataset = json.loads(line)\n",
    "                    images = dataset['images']\n",
    "            box_ap, res, iter_times = evaluate(yolo_pred,\n",
    "                                               images,\n",
    "                                               val_coco_root,\n",
    "                                               val_annotate,\n",
    "                                               eval_batch_size,\n",
    "                                               clsid2catid,\n",
    "                                               eval_batch_size * user_batch_size, \n",
    "                                               num_cores)\n",
    "\n",
    "        iter_ds = pd.concat([iter_ds, pd.DataFrame(iter_times, columns=[col_name(opt)])], axis=1)\n",
    "        results = pd.concat([results, res], axis=1)\n",
    "    display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_tensorflow_p36_2",
   "language": "python",
   "name": "aws_neuron_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
