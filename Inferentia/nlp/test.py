from transformers import pipeline
import tensorflow as tf
import tensorflow.neuron as tfn
import time

#model_name = 'facebook/bart-large-mnli'
# 'typeform/distilbert-base-uncased-mnli' is unsupported 
# (see SUPPORTED_TASKS in https://huggingface.co/transformers/_modules/transformers/pipelines.html)
#model_name = 'typeform/distilbert-base-uncased-mnli'
# Choosing supported model for 'zero-shot-classification' task
model_name = 'roberta-large-mnli'
pipe = pipeline('zero-shot-classification', model=model_name, framework='tf')

sequence_to_classify = "one day I will see the world"

# 52 labels (classes)
# 1 million sequences 
# 128 seqlen (varies 5 to 128)

# stats for typeform/distilbert-base-uncased-mnli
# g4dn.2xlarge: 5 minutes 24 sec for 10k sequences
# p2.xlarge: 11 minutes 42 sec for 10k sequences
candidate_labels = ['travel', 'cooking', 'dancing']

start = time.time()
print(pipe(sequence_to_classify, candidate_labels))
print("CPU infer time: ", time.time() - start)

# On g4 machine, DLAMI v48, pytorch_p36
#(pytorch_p36) ubuntu@ip-172-31-6-163:~/github$ python test.py
#{'sequence': 'one day I will see the world', 'labels': ['travel', 'dancing', 'cooking'], 'scores': [0.9938651323318481, 0.003273785812780261, 0.002861040411517024]}


neuron_pipe = pipeline('zero-shot-classification', model='roberta-large-mnli', framework='tf')

#the first step is to modify the underlying tokenizer to create a static
#input shape as inferentia does not work with dynamic input shapes
original_tokenizer = pipe.tokenizer

#we intercept the function call to the original tokenizer
#and inject our own code to modify the arguments
def wrapper_function(*args, **kwargs):
    kwargs['padding'] = 'max_length'
    #this is the key line here to set a static input shape
    #so that all inputs are set to a len of 128
    kwargs['max_length'] = 128
    kwargs['truncation'] = True
    kwargs['return_tensors'] = 'tf'
    return original_tokenizer(*args, **kwargs)

#insert our wrapper function as the new tokenizer as well
#as reinserting back some attribute information that was lost
#when we replaced the original tokenizer with our wrapper function
neuron_pipe.tokenizer = wrapper_function
neuron_pipe.tokenizer.decode = original_tokenizer.decode
neuron_pipe.tokenizer.mask_token_id = original_tokenizer.mask_token_id
neuron_pipe.tokenizer.pad_token_id = original_tokenizer.pad_token_id
neuron_pipe.tokenizer.convert_ids_to_tokens = original_tokenizer.convert_ids_to_tokens

#Now that our neuron_classifier is ready we can use it to
#generate an example input which is needed to compile the model
#note that pipe.model is the actual underlying model itself which
#is what Tensorflow Neuron actually compiles.
example_inputs = neuron_pipe.tokenizer('we can use any string here to generate example inputs')
#compile the model by calling tfn.trace by passing in the underlying model
#and the example inputs generated by our updated tokenizer
start = time.time()
neuron_model = tfn.trace(pipe.model, example_inputs)
print("Neuron compile time: ", time.time() - start)
#saved_model_dir = './neuron-' + model_name
#neuron_model.save(saved_model_dir)
#tf.keras.models.load_model(saved_model_dir)

#now we can insert the neuron_model and replace the cpu model

#so now we have a huggingface pipeline that uses and underlying neuron model!
neuron_pipe.model = neuron_model
neuron_pipe.model.config = pipe.model.config

start = time.time()
print(neuron_pipe(sequence_to_classify, candidate_labels))
print("Neuron infer time: ", time.time() - start)
